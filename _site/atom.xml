<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>6.S898 Deep Learning Blogs 2023</title>
 <link href="http://0.0.0.0:8080/staging/atom.xml" rel="self"/>
 <link href="http://0.0.0.0:8080/staging/"/>
 <updated>2023-12-12T01:39:55-06:00</updated>
 <id>http://0.0.0.0:8080</id>
 <author>
   <name></name>
   <email></email>
 </author>

 
 <entry>
   <title>Iterated Representation Learning</title>
   <link href="http://0.0.0.0:8080/blog/2023/copy/"/>
   <updated>2023-11-09T00:00:00-06:00</updated>
   <id>http://0.0.0.0:8080/staging/blog/2023/copy</id>
   <content type="html">&lt;h2 id=&quot;outline&quot;&gt;Outline&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Background
    &lt;ul&gt;
      &lt;li&gt;Representation primer&lt;/li&gt;
      &lt;li&gt;
        &lt;ul&gt;
          &lt;li&gt;What is representation?
Why is it important to learn well (properties of good representations and its utility)?
Autoencoder primer
What is an autoencoder (AE) and how does it relate to representation?&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Iterated Representation Learning Framework
AEs (deterministic reconstruction)
Step 1: Given some dataset, use an AE to learn its embedding space.
Step 2: Using the learned embedding and AE, reconstruct the original dataset and compute the reconstruction loss.
Step 3: Using the reconstructed dataset, repeat Steps 1 and 2, iterating as long as desired.
VAEs (generative modeling)
Step 1: Given some dataset, use a VAE to learn its embedding space.
Step 2: Using the learned embedding and VAE, generate a new dataset.
Step 3: Using the newly generated dataset, repeat Steps 1 and 2, iterating as long as desired.&lt;/p&gt;

&lt;p&gt;Potential Questions / Hypotheses
Following the iterated representation learning framework above, can we iterate until we reach some kind of convergence with respect to the model and/or learned embedding space? 
If so, can this tell us any properties of the representation space, learned representation, model, and/or data? 
Does the number of iterations until convergence have anything to do with how “good” or stable the model or learned representation is?
In the deterministic autoencoder case, how do the reconstruction losses perform as iterations go on? Do we converge? How quickly? If the loss seems to diverge (relative to the original data), does it diverge linearly, exponentially, etc.?
What can we say about characteristics of the data that are maintained through iterations, and characteristics that evolve as the iterations go on? For example, if we observe that a model remains invariant to a certain feature, but becomes sensitive to new features of the data, what does this tell us about these particular features, our model, and the original data itself? Are there any other patterns we can identify along these lines?
Can we propose some sort of representation learning evaluation framework using iterated representation learning, e.g. rough guidelines on ideal number of iterations required until convergence, and what this says about how good a model is?&lt;/p&gt;

&lt;p&gt;Future Work
How can we make iterated representation learning more computationally tractable? 
Can any of these results be generalized to other types of deep learning models?
Are there any theoretical guarantees we can prove?&lt;/p&gt;

&lt;h2 id=&quot;equations&quot;&gt;Equations&lt;/h2&gt;

&lt;p&gt;This theme supports rendering beautiful math in inline and display modes using &lt;a href=&quot;https://www.mathjax.org/&quot;&gt;MathJax 3&lt;/a&gt; engine.
You just need to surround your math expression with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$&lt;/code&gt;, like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$ E = mc^2 $$&lt;/code&gt;.
If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).&lt;/p&gt;

&lt;p&gt;To use display mode, again surround your expression with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$&lt;/code&gt; and place it as a separate paragraph.
Here is an example:&lt;/p&gt;

\[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\]

&lt;p&gt;Note that MathJax 3 is &lt;a href=&quot;https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html&quot;&gt;a major re-write of MathJax&lt;/a&gt; that brought a significant improvement to the loading and rendering speed, which is now &lt;a href=&quot;http://www.intmath.com/cg5/katex-mathjax-comparison.php&quot;&gt;on par with KaTeX&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;images-and-figures&quot;&gt;Images and Figures&lt;/h2&gt;

&lt;p&gt;Its generally a better idea to avoid linking to images hosted elsewhere - links can break and you
might face losing important information in your blog post.
To include images in your submission in this way, you must do something like the following:&lt;/p&gt;

&lt;div class=&quot;language-markdown highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{% include figure.html path=&quot;assets/img/2022-12-01-distill-example/iclr.png&quot; class=&quot;img-fluid&quot; %}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;which results in the following image:&lt;/p&gt;

&lt;figure&gt;

  &lt;picture&gt;
    
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2022-12-01-distill-example/iclr-480.webp&quot; /&gt;
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2022-12-01-distill-example/iclr-800.webp&quot; /&gt;
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2022-12-01-distill-example/iclr-1400.webp&quot; /&gt;
    

    &lt;!-- Fallback to the original file --&gt;
    &lt;img src=&quot;/staging/assets/img/2022-12-01-distill-example/iclr.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt;

  &lt;/picture&gt;

&lt;/figure&gt;

&lt;p&gt;To ensure that there are no namespace conflicts, you must save your asset to your unique directory
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;/assets/img/2023-05-01-[SUBMISSION NAME]&lt;/code&gt; within your submission.&lt;/p&gt;

&lt;p&gt;Please avoid using the direct markdown method of embedding images; they may not be properly resized.
Some more complex ways to load images (note the different styles of the shapes/shadows):&lt;/p&gt;

&lt;div class=&quot;row mt-3&quot;&gt;
    &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt;
        &lt;figure&gt;

  &lt;picture&gt;
    
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2022-12-01-distill-example/9-480.webp&quot; /&gt;
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2022-12-01-distill-example/9-800.webp&quot; /&gt;
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2022-12-01-distill-example/9-1400.webp&quot; /&gt;
    

    &lt;!-- Fallback to the original file --&gt;
    &lt;img src=&quot;/staging/assets/img/2022-12-01-distill-example/9.jpg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt;

  &lt;/picture&gt;

&lt;/figure&gt;

    &lt;/div&gt;
    &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt;
        &lt;figure&gt;

  &lt;picture&gt;
    
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2022-12-01-distill-example/7-480.webp&quot; /&gt;
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2022-12-01-distill-example/7-800.webp&quot; /&gt;
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2022-12-01-distill-example/7-1400.webp&quot; /&gt;
    

    &lt;!-- Fallback to the original file --&gt;
    &lt;img src=&quot;/staging/assets/img/2022-12-01-distill-example/7.jpg&quot; class=&quot;img-fluid rounded z-depth-1&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt;

  &lt;/picture&gt;

&lt;/figure&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;caption&quot;&gt;
    A simple, elegant caption looks good between image rows, after each row, or doesn&apos;t have to be there at all.
&lt;/div&gt;

&lt;div class=&quot;row mt-3&quot;&gt;
    &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt;
        &lt;figure&gt;

  &lt;picture&gt;
    
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2022-12-01-distill-example/8-480.webp&quot; /&gt;
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2022-12-01-distill-example/8-800.webp&quot; /&gt;
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2022-12-01-distill-example/8-1400.webp&quot; /&gt;
    

    &lt;!-- Fallback to the original file --&gt;
    &lt;img src=&quot;/staging/assets/img/2022-12-01-distill-example/8.jpg&quot; class=&quot;img-fluid z-depth-2&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt;

  &lt;/picture&gt;

&lt;/figure&gt;

    &lt;/div&gt;
    &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt;
        &lt;figure&gt;

  &lt;picture&gt;
    
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2022-12-01-distill-example/10-480.webp&quot; /&gt;
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2022-12-01-distill-example/10-800.webp&quot; /&gt;
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2022-12-01-distill-example/10-1400.webp&quot; /&gt;
    

    &lt;!-- Fallback to the original file --&gt;
    &lt;img src=&quot;/staging/assets/img/2022-12-01-distill-example/10.jpg&quot; class=&quot;img-fluid z-depth-2&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt;

  &lt;/picture&gt;

&lt;/figure&gt;

    &lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;row mt-3&quot;&gt;
    &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt;
        &lt;figure&gt;

  &lt;picture&gt;
    
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2022-12-01-distill-example/11-480.webp&quot; /&gt;
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2022-12-01-distill-example/11-800.webp&quot; /&gt;
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2022-12-01-distill-example/11-1400.webp&quot; /&gt;
    

    &lt;!-- Fallback to the original file --&gt;
    &lt;img src=&quot;/staging/assets/img/2022-12-01-distill-example/11.jpg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt;

  &lt;/picture&gt;

&lt;/figure&gt;

    &lt;/div&gt;
    &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt;
        &lt;figure&gt;

  &lt;picture&gt;
    
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2022-12-01-distill-example/12-480.webp&quot; /&gt;
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2022-12-01-distill-example/12-800.webp&quot; /&gt;
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2022-12-01-distill-example/12-1400.webp&quot; /&gt;
    

    &lt;!-- Fallback to the original file --&gt;
    &lt;img src=&quot;/staging/assets/img/2022-12-01-distill-example/12.jpg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt;

  &lt;/picture&gt;

&lt;/figure&gt;

    &lt;/div&gt;
    &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt;
        &lt;figure&gt;

  &lt;picture&gt;
    
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2022-12-01-distill-example/7-480.webp&quot; /&gt;
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2022-12-01-distill-example/7-800.webp&quot; /&gt;
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2022-12-01-distill-example/7-1400.webp&quot; /&gt;
    

    &lt;!-- Fallback to the original file --&gt;
    &lt;img src=&quot;/staging/assets/img/2022-12-01-distill-example/7.jpg&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt;

  &lt;/picture&gt;

&lt;/figure&gt;

    &lt;/div&gt;
&lt;/div&gt;

&lt;h3 id=&quot;interactive-figures&quot;&gt;Interactive Figures&lt;/h3&gt;

&lt;p&gt;Here’s how you could embed interactive figures that have been exported as HTML files.
Note that we will be using plotly for this demo, but anything built off of HTML should work
(&lt;strong&gt;no extra javascript is allowed!&lt;/strong&gt;).
All that’s required is for you to export your figure into HTML format, and make sure that the file
exists in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;assets/html/[SUBMISSION NAME]/&lt;/code&gt; directory in this repository’s root directory.
To embed it into any page, simply insert the following code anywhere into your page.&lt;/p&gt;

&lt;div class=&quot;language-markdown highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{% include [FIGURE_NAME].html %} 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For example, the following code can be used to generate the figure underneath it.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;plotly.express&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;px&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;px&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;density_mapbox&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;Latitude&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lon&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;Longitude&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;Magnitude&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;radius&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;center&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lon&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;180&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;zoom&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mapbox_style&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;stamen-terrain&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write_html&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;./assets/html/2022-12-01-distill-example/plotly_demo_1.html&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And then include it with the following:&lt;/p&gt;

&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;l-page&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;iframe&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;src=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;{{ &apos;assets/html/2022-12-01-distill-example/plotly_demo_1.html&apos; | relative_url }}&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;frameborder=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;0&apos;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;scrolling=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&apos;no&apos;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;height=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;600px&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;width=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;100%&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&amp;lt;/iframe&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Voila!&lt;/p&gt;

&lt;div class=&quot;l-page&quot;&gt;
  &lt;iframe src=&quot;/staging/assets/html/2022-12-01-distill-example/plotly_demo_1.html&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot; height=&quot;600px&quot; width=&quot;100%&quot;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h2 id=&quot;citations&quot;&gt;Citations&lt;/h2&gt;

&lt;p&gt;Citations are then used in the article body with the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;d-cite&amp;gt;&lt;/code&gt; tag.
The key attribute is a reference to the id provided in the bibliography.
The key attribute can take multiple ids, separated by commas.&lt;/p&gt;

&lt;p&gt;The citation is presented inline like this: &lt;d-cite key=&quot;gregor2015draw&quot;&gt;&lt;/d-cite&gt; (a number that displays more information on hover).
If you have an appendix, a bibliography is automatically created and populated in it.&lt;/p&gt;

&lt;p&gt;Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover.
However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h2&gt;

&lt;p&gt;Just wrap the text you would like to show up in a footnote in a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;d-footnote&amp;gt;&lt;/code&gt; tag.
The number of the footnote will be automatically generated.&lt;d-footnote&gt;This will become a hoverable footnote.&lt;/d-footnote&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;code-blocks&quot;&gt;Code Blocks&lt;/h2&gt;

&lt;p&gt;This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting.
It supports more than 100 languages.
This example is in C++.
All you have to do is wrap your code in a liquid tag:&lt;/p&gt;

&lt;p&gt;{% highlight c++ linenos %}  &lt;br /&gt; code code code &lt;br /&gt; {% endhighlight %}&lt;/p&gt;

&lt;p&gt;The keyword &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;linenos&lt;/code&gt; triggers display of line numbers. You can try toggling it on or off yourself below:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-c--&quot; data-lang=&quot;c++&quot;&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;argc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[])&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;string&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;myString&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;input a string: &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;getline&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;myString&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;length&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;myString&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;

    &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;charArray&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;char&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;charArray&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;myString&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;cout&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;charArray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;diagrams&quot;&gt;Diagrams&lt;/h2&gt;

&lt;p&gt;This theme supports generating various diagrams from a text description using &lt;a href=&quot;https://github.com/zhustec/jekyll-diagrams&quot; target=&quot;\_blank&quot;&gt;jekyll-diagrams&lt;/a&gt; plugin.
Below, we generate a few examples of such diagrams using languages such as &lt;a href=&quot;https://mermaid-js.github.io/mermaid/&quot; target=&quot;\_blank&quot;&gt;mermaid&lt;/a&gt;, &lt;a href=&quot;https://plantuml.com/&quot; target=&quot;\_blank&quot;&gt;plantuml&lt;/a&gt;, &lt;a href=&quot;https://vega.github.io/vega-lite/&quot; target=&quot;\_blank&quot;&gt;vega-lite&lt;/a&gt;, etc.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; different diagram-generation packages require external dependencies to be installed on your machine.
Also, be mindful of that because of diagram generation the fist time you build your Jekyll website after adding new diagrams will be SLOW.
For any other details, please refer to &lt;a href=&quot;https://github.com/zhustec/jekyll-diagrams&quot; target=&quot;\_blank&quot;&gt;jekyll-diagrams&lt;/a&gt; README.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This is not supported for local rendering!&lt;/p&gt;

&lt;p&gt;The diagram below was generated by the following code:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{% mermaid %}
sequenceDiagram
    participant John
    participant Alice
    Alice-&amp;gt;&amp;gt;John: Hello John, how are you?
    John--&amp;gt;&amp;gt;Alice: Great!
{% endmermaid %}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;jekyll-diagrams diagrams mermaid&quot;&gt;
  Command Not Found: mmdc
&lt;/div&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;blockquotes&quot;&gt;Blockquotes&lt;/h2&gt;

&lt;blockquote&gt;
    We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another.
    —Anais Nin
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;layouts&quot;&gt;Layouts&lt;/h2&gt;

&lt;p&gt;The main text column is referred to as the body.
It is the assumed layout of any direct descendants of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;d-article&lt;/code&gt; element.&lt;/p&gt;

&lt;div class=&quot;fake-img l-body&quot;&gt;
  &lt;p&gt;.l-body&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;For images you want to display a little larger, try &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.l-page&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;fake-img l-page&quot;&gt;
  &lt;p&gt;.l-page&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;All of these have an outset variant if you want to poke out from the body text a little bit.
For instance:&lt;/p&gt;

&lt;div class=&quot;fake-img l-body-outset&quot;&gt;
  &lt;p&gt;.l-body-outset&lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;fake-img l-page-outset&quot;&gt;
  &lt;p&gt;.l-page-outset&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Occasionally you’ll want to use the full browser width.
For this, use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.l-screen&lt;/code&gt;.
You can also inset the element a little from the edge of the browser by using the inset variant.&lt;/p&gt;

&lt;div class=&quot;fake-img l-screen&quot;&gt;
  &lt;p&gt;.l-screen&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;fake-img l-screen-inset&quot;&gt;
  &lt;p&gt;.l-screen-inset&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;The final layout is for marginalia, asides, and footnotes.
It does not interrupt the normal flow of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.l-body&lt;/code&gt; sized text except on mobile screen sizes.&lt;/p&gt;

&lt;div class=&quot;fake-img l-gutter&quot;&gt;
  &lt;p&gt;.l-gutter&lt;/p&gt;
&lt;/div&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;other-typography&quot;&gt;Other Typography?&lt;/h2&gt;

&lt;p&gt;Emphasis, aka italics, with &lt;em&gt;asterisks&lt;/em&gt; (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;*asterisks*&lt;/code&gt;) or &lt;em&gt;underscores&lt;/em&gt; (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_underscores_&lt;/code&gt;).&lt;/p&gt;

&lt;p&gt;Strong emphasis, aka bold, with &lt;strong&gt;asterisks&lt;/strong&gt; or &lt;strong&gt;underscores&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Combined emphasis with &lt;strong&gt;asterisks and &lt;em&gt;underscores&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Strikethrough uses two tildes. &lt;del&gt;Scratch this.&lt;/del&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;First ordered list item&lt;/li&gt;
  &lt;li&gt;Another item
⋅⋅* Unordered sub-list.&lt;/li&gt;
  &lt;li&gt;Actual numbers don’t matter, just that it’s a number
⋅⋅1. Ordered sub-list&lt;/li&gt;
  &lt;li&gt;And another item.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;⋅⋅⋅You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).&lt;/p&gt;

&lt;p&gt;⋅⋅⋅To have a line break without a paragraph, you will need to use two trailing spaces.⋅⋅
⋅⋅⋅Note that this line is separate, but within the same paragraph.⋅⋅
⋅⋅⋅(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Unordered list can use asterisks&lt;/li&gt;
  &lt;li&gt;Or minuses&lt;/li&gt;
  &lt;li&gt;Or pluses&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://www.google.com&quot;&gt;I’m an inline-style link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.google.com&quot; title=&quot;Google&apos;s Homepage&quot;&gt;I’m an inline-style link with title&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.mozilla.org&quot;&gt;I’m a reference-style link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;../blob/master/LICENSE&quot;&gt;I’m a relative reference to a repository file&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://slashdot.org&quot;&gt;You can use numbers for reference-style link definitions&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Or leave it empty and use the &lt;a href=&quot;http://www.reddit.com&quot;&gt;link text itself&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;URLs and URLs in angle brackets will automatically get turned into links. 
http://www.example.com or &lt;a href=&quot;http://www.example.com&quot;&gt;http://www.example.com&lt;/a&gt; and sometimes 
example.com (but not on Github, for example).&lt;/p&gt;

&lt;p&gt;Some text to show that the reference links can follow later.&lt;/p&gt;

&lt;p&gt;Here’s our logo (hover to see the title text):&lt;/p&gt;

&lt;p&gt;Inline-style: 
&lt;img src=&quot;https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png&quot; alt=&quot;alt text&quot; title=&quot;Logo Title Text 1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Reference-style: 
&lt;img src=&quot;https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png&quot; alt=&quot;alt text&quot; title=&quot;Logo Title Text 2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Inline &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;code&lt;/code&gt; has &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;back-ticks around&lt;/code&gt; it.&lt;/p&gt;

&lt;div class=&quot;language-javascript highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;JavaScript syntax highlighting&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;nx&quot;&gt;alert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Python syntax highlighting&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;No language indicated, so no syntax highlighting. 
But let&apos;s throw in a &amp;lt;b&amp;gt;tag&amp;lt;/b&amp;gt;.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Colons can be used to align columns.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Tables&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Are&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Cool&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;col 3 is&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;right-aligned&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;$1600&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;col 2 is&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;centered&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;$12&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;zebra stripes&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;are neat&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;$1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;There must be at least 3 dashes separating each header cell.
The outer pipes (|) are optional, and you don’t need to make the 
raw Markdown line up prettily. You can also use inline Markdown.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Markdown&lt;/th&gt;
      &lt;th&gt;Less&lt;/th&gt;
      &lt;th&gt;Pretty&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;em&gt;Still&lt;/em&gt;&lt;/td&gt;
      &lt;td&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;renders&lt;/code&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;nicely&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;blockquote&gt;
  &lt;p&gt;Blockquotes are very handy in email to emulate reply text.
This line is part of the same quote.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Quote break.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can &lt;em&gt;put&lt;/em&gt; &lt;strong&gt;Markdown&lt;/strong&gt; into a blockquote.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Here’s a line for us to start with.&lt;/p&gt;

&lt;p&gt;This line is separated from the one above by two newlines, so it will be a &lt;em&gt;separate paragraph&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;This line is also a separate paragraph, but…
This line is only separated by a single newline, so it’s a separate line in the &lt;em&gt;same paragraph&lt;/em&gt;.&lt;/p&gt;
</content>
 </entry>
 
 <entry>
   <title>Iterated Representation Learning</title>
   <link href="http://0.0.0.0:8080/blog/2023/Iterated-Representation-Learning/"/>
   <updated>2023-11-09T00:00:00-06:00</updated>
   <id>http://0.0.0.0:8080/staging/blog/2023/Iterated-Representation-Learning</id>
   <content type="html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Representation learning has become a transformative subfield of deep learning within recent years, garnering widespread attention for its sophistication in learning lower-dimensional embeddings of data beyond classical techniques such as principal component analysis (PCA). From class, we learned that desirable characteristics of good representations include minimality, sufficiency, disentangelement, and interpretability. However, because typical representation learning techniques such as autoencoders learn only one latent embedding from the input data, there exists a gap in the literature on the &lt;em&gt;stability&lt;/em&gt; of the model and learned embeddings.&lt;/p&gt;

&lt;p&gt;In this project, we thus explore a new approach to traditional representation learning techniques, in which embeddings for a given set of data are learned repeatedly until some sort of convergence with respect to the model and learned embedding space, a process we call &lt;strong&gt;Iterated Representation Learning (IRL)&lt;/strong&gt;; by analyzing the performance of this iterative approach, our work aims to discover potential insights into the robustness qualities inherent to a model and its associated latent embedding space. We propose an algorithmic framework for IRL, provide an empirical case study of the efficacy of our IRL framework on the MNIST dataset, and suggest a novel evaluation procedure for representation stability and robustness via iterated learning.&lt;/p&gt;

&lt;h3 id=&quot;representation-learning-primer&quot;&gt;Representation Learning Primer&lt;/h3&gt;

&lt;p&gt;The goal of representation learning is to build models that effectively learn meaningful representations of the data. Representations are important for a variety of reasons, including determining which features are the most explanatory or variable in a dataset, compressing repeated information from a dataset to make it more compact, and learning more effective neural networks, to name a few examples. These representations are typically abstract and less interpretable than the input data, but of lower dimension, which makes them useful in capturing the most essential or compressed characteristics of the data.&lt;/p&gt;

&lt;p&gt;More formally, representation learning aims to learn a mapping from datapoints \(\mathbf{x} \in \mathcal{X}\) to a (typically lower-dimensional) representation \(\mathbf{z} \in \mathcal{Z}\); we call this mapping an &lt;strong&gt;encoding&lt;/strong&gt;, and the learned encoding is a function \(f: \mathcal{X} \rightarrow \mathcal{Z}\). From this, a &lt;strong&gt;decoder&lt;/strong&gt; \(g: \mathcal{Z} \rightarrow \mathcal{X}\) can be applied to reconstruct the encoded data into its original dimension. This is demonstrated in the diagram below.&lt;/p&gt;

&lt;figure&gt;

  &lt;picture&gt;
    
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/representation-480.webp&quot; /&gt;
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/representation-800.webp&quot; /&gt;
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/representation-1400.webp&quot; /&gt;
    

    &lt;!-- Fallback to the original file --&gt;
    &lt;img src=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/representation.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt;

  &lt;/picture&gt;

&lt;/figure&gt;

&lt;div class=&quot;caption&quot;&gt;
    Representation learning goal. Image credit: &lt;i&gt;Foundations of Computer Vision: Representation Learning&lt;/i&gt; (Torralba, Isola, Freeman 2023).
&lt;/div&gt;

&lt;p&gt;Some of the most salient learning methods within representation learning today include autoencoding, contrastive learning, clustering, and imputation; in this project, we focus on specifically on iterative approaches for the class of &lt;strong&gt;autoencoders&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Representation learning also has intricate ties to generative modeling, the subfield of deep learning that aims to generate new data by mapping a simple base distribution to complicated high-dimensional data, which is essentially the opposite goal of representation learning. Then, after learning an embedding space via representation learning, this embedding can then be sampled from to &lt;em&gt;generate&lt;/em&gt; new data that mimics the original data, as demonstrated by &lt;strong&gt;variational autoencoders (VAEs)&lt;/strong&gt;, which we also explore in this paper.&lt;/p&gt;

&lt;h3 id=&quot;prior-literature&quot;&gt;Prior Literature&lt;/h3&gt;

&lt;p&gt;Relatively little literature exists regarding iteratively training dimensionality reduction or representation learning models. &lt;a href=&quot;https://ieeexplore.ieee.org/document/9528915&quot;&gt;Vlahek and Mongus (2023)&lt;/a&gt; proposes an iterative approach for &lt;em&gt;conducting&lt;/em&gt; representation learning more efficiently, specifically for the goal of learning the most salient features, which fundamentally diverges from our goal and also does not consider embedding robustness. &lt;a href=&quot;https://arxiv.org/abs/1809.10324&quot;&gt;Chen et al. (2019)&lt;/a&gt; introduces an iterative model for supervised extractive text summarization, though their objective of trying to optimize for a particular document by feeding a given document through the representation multiple times differs from ours. &lt;a href=&quot;https://ieeexplore.ieee.org/abstract/document/9414713&quot;&gt;Cai, Wang, and Li (2021)&lt;/a&gt; finds an iterative framework for self-supervised speaker representation learning which performs 61% better than a speaker embedding model trained with contrastive loss, but mainly focuses on the self-supervision aspect of the model and optimizes purely for model test accuracy, not considering other metrics such as stability or robustness.&lt;/p&gt;

&lt;p&gt;Overall, we find that the literature regarding iterative approaches to representation learning is already sparse; of the work that exists, most focuses on very specific use cases, and no work directly examines the robustness or stability of the model and embeddings themselves learned over time, rather optimizing purely for final model performance.&lt;/p&gt;

&lt;h2 id=&quot;iterated-representation-learning&quot;&gt;Iterated Representation Learning&lt;/h2&gt;

&lt;h3 id=&quot;existing-dimensionality-reduction-and-representation-models&quot;&gt;Existing Dimensionality Reduction and Representation Models&lt;/h3&gt;

&lt;p&gt;Nowadays, there are a variety of approaches to effective dimensionality reduction. Below we cover three of the most common techniques.&lt;/p&gt;

&lt;h4 id=&quot;principal-component-analysis&quot;&gt;Principal Component Analysis&lt;/h4&gt;

&lt;p&gt;Principal Component Analysis (PCA) has two primary objectives. First, maximizing sample variance of the newly transformed data, which is analogous to identifying and capturing the greatest (largest) directions of variability in the data (principal components or PCs). Formally, a PC is defined&lt;/p&gt;

\[v^* = \arg \max_v \frac{1}{N-1} \sum_{n=1}^N (x^T_n v - \bar{x}^T v)^2 = \arg \max_v v^T C v\]

&lt;p&gt;where \(C = \frac{X^T X}{n-1} \in \mathbb{R}^{d \times d}\) is the empirical covariance matrix.&lt;/p&gt;

&lt;p&gt;The second objective is minimizing reconstruction loss, which is analogous to identifying the directions of variability to accurately and concisely represent data. Let \(U\) be the orthonormal basis projection matrix of eigenvectors of \(C\). Then we define reconstruction loss as&lt;/p&gt;

\[\mathcal{L}(U) = \frac{\sum_{n=1}^N ||x_n - U U^T x_n||^2}{N}\]

&lt;p&gt;Above, we observe that maximizing sample variance and minimizing reconstruction loss go hand-in-hand. Since PCA applies projections by multiplying vectors/matrices to the data, PCA is limited to the &lt;em&gt;linear&lt;/em&gt; transformation setting, hence restricting its applicability in many modeling problems.&lt;/p&gt;

&lt;h4 id=&quot;autoencoders&quot;&gt;Autoencoders&lt;/h4&gt;

&lt;figure&gt;

  &lt;picture&gt;
    
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/ae-480.webp&quot; /&gt;
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/ae-800.webp&quot; /&gt;
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/ae-1400.webp&quot; /&gt;
    

    &lt;!-- Fallback to the original file --&gt;
    &lt;img src=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/ae.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt;

  &lt;/picture&gt;

&lt;/figure&gt;

&lt;div class=&quot;caption&quot;&gt;
    Autoencoder structure. Image credit: &lt;i&gt;Foundations of Computer Vision: Representation Learning&lt;/i&gt; (Torralba, Isola, Freeman 2023).
&lt;/div&gt;

&lt;p&gt;Similar to PCA, autoencoders also aim to minimize reconstruction loss. However, autoencoders are not limited to just linear transformations, which enables autoencoders to learn more general lower-dimensional representations of data. Autoencoders are comprised of an encoder and decoder, where the encoder maps data to a lower-dimensional representation (embedding) via some function $f$, and the decoder maps the originally transformed data back to its original dimensional space via some function $g$.&lt;/p&gt;

&lt;p&gt;End to end, the data space starts in \(\mathbb{R}^N\), is downsized to \(\mathbb{R}^M\) by \(f\), and then is reverted back to \(\mathbb{R}^N\) where \(N &amp;gt; M\). In this case, we can formalize the objective as follows:&lt;/p&gt;

\[f^*, g^* = \arg \min_{f,g} E_\mathbf{x} || \mathbf{x} - g(f(\mathbf{x}))||^2_2\]

&lt;h4 id=&quot;variational-autoencoders&quot;&gt;Variational Autoencoders&lt;/h4&gt;

&lt;p&gt;VAEs couple autoencoders with probability to get maximum likelihood generative models. Typically for encoding, VAEs regularizes the latent (hidden) distribution of data to “massage” the distribution into a unit Gaussian, and when reverting back to the original dimensional space, VAEs add noise to the output — hence, a mixture of Gaussians. By imposing a unit Gaussian structure on the learned embedding space, this allows VAEs to act as generative models by sampling from the Gaussian latent space to generate new data. Unlike traditional autoencoders, VAEs may have embedding spaces that are complicated (if not just as complicated as the data).&lt;/p&gt;

&lt;figure&gt;

  &lt;picture&gt;
    
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/vae-480.webp&quot; /&gt;
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/vae-800.webp&quot; /&gt;
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/vae-1400.webp&quot; /&gt;
    

    &lt;!-- Fallback to the original file --&gt;
    &lt;img src=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/vae.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt;

  &lt;/picture&gt;

&lt;/figure&gt;

&lt;div class=&quot;caption&quot;&gt;
    VAE&apos;s complex embedding space. Image credit: &lt;i&gt;Foundations of Computer Vision: Generative Modeling Meets Representation Learning&lt;/i&gt; (Torralba, Isola, Freeman 2023).
&lt;/div&gt;

&lt;p&gt;Formally, the VAE learning problem is defined by&lt;/p&gt;

\[\theta^* = \arg \max_{\theta} L(\{\mathbf{x}^{(i)}\}^N_{i=1}, \theta) = \arg \max_{\theta} \sum_{i=1}^N \log \int_{\mathbf{z}} \mathcal{N} (\mathbf{x}^{(i)}; g_{\theta}^{\mu}(\mathbf{z}), g_{\theta}^{\Sigma}(\mathbf{z})) \cdot \mathcal{N}(\mathbf{z}; \mathbf{0}, \mathbf{\mathrm{I}}) d\mathbf{z}\]

&lt;h3 id=&quot;iterated-representation-learning-1&quot;&gt;Iterated Representation Learning&lt;/h3&gt;

&lt;h4 id=&quot;proposed-framework&quot;&gt;Proposed Framework&lt;/h4&gt;

&lt;p&gt;We now introduce the Iterated Representation Learning Framework (IRL) for autoencoders and VAEs. We start with IRL for autoencoders:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Given design matrix \(X\), learn an autoencoder for \(X\).&lt;/li&gt;
  &lt;li&gt;Using the decoder from above, reconstruct the data to get \(X&apos;\) and compute its reconstruction loss.&lt;/li&gt;
  &lt;li&gt;Using the reconstructed data \(X&apos;\), repeat Steps 1 and 2 and iterate until the reconstruction loss converges or reaching iteration limit.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As for VAEs, we follow a similar procedure as above.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Given design matrix \(X\), learn a VAE for \(X\).&lt;/li&gt;
  &lt;li&gt;Using the decoder and adding Gaussian noise, reconstruct the data to get \(X&apos;\). Compute its reconstruction loss.&lt;/li&gt;
  &lt;li&gt;Using the reconstructed data \(X&apos;\), repeat Steps 1 and 2 and iterate until the reconstruction loss converges or reaching iteration limit.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this report, we examine how IRL is connected to representation, investigate several hypotheses about IRL, and conduct a preliminary case study of IRL on the MNIST dataset.&lt;/p&gt;

&lt;h4 id=&quot;preliminary-questions-and-hypotheses&quot;&gt;Preliminary Questions and Hypotheses&lt;/h4&gt;

&lt;p&gt;Motivated by how there may be unexplored stability properties of embeddings, our main hypotheses are twofold. First,  iterated reconstruction loss per IRL can convergence with respect to the model. Second, learned embedding spaces can be reached via IRL, and that the number of iterations until convergence, loss at convergence, and such preserved features upon convergence could reveal meaningful properties of the true representation space, model, and data that are not immediately obvious from a standard autoencoder model.&lt;/p&gt;

&lt;p&gt;More specifically, does the number of iterations until convergence have anything to do with how ``good’’ or stable the model or learned representation is? What does it mean if the reconstruction losses converge? What can we say about characteristics of the data that are maintained through iterations, and characteristics that evolve as the iterations go on? For example, if we observe that a model remains invariant to a certain feature, but becomes sensitive to new features of the data, what does this tell us about these particular features, our model, and the original data itself?&lt;/p&gt;

&lt;p&gt;Perhaps most importantly, beyond the qualitative observations themselves, can we propose some sort of representation learning evaluation framework using iterated representation learning, e.g. rough guidelines on ideal number of iterations required until convergence, and what this says about how good a model is? Ultimately, we hope that using an iterated framework can serve as a general tool for (1) evaluating the stability or robustness of a representation learning model and (2) identifying the most core characteristics of a given dataset.&lt;/p&gt;

&lt;h2 id=&quot;case-study-mnist-dataset&quot;&gt;Case Study: MNIST Dataset&lt;/h2&gt;

&lt;p&gt;To evaluate IRL on a real-world dataset, we selected MNIST to test our hypotheses. We carefully designed our experiments, collected relevant data, and include our analysis below.&lt;/p&gt;

&lt;h3 id=&quot;experimental-design&quot;&gt;Experimental Design&lt;/h3&gt;

&lt;p&gt;For our experiments, we implemented IRL using the framework given above for the class MNIST digits dataset (due to its simplicity and intrepretability), where we preset the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;num_iterations&lt;/code&gt;. At every iteration, we initialize a new autoencoder model with &lt;a href=&quot;https://arxiv.org/abs/2206.08309&quot;&gt;Chadebec, Vincent, and Allassonnière’s (2022)&lt;/a&gt; &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pythae&lt;/code&gt; autoencoder/VAE library. The encoder architecture is formed by sequential convolutional layers from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;PyTorch&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;We then trained the model, reconstructed the data, and saved the trained and validation loss. We also saved the original train/test and reconstructed train/test images of the first 25 datapoints to track how IRL progressed visually.&lt;/p&gt;

&lt;h3 id=&quot;autoencoder-irl-analysis&quot;&gt;Autoencoder IRL Analysis&lt;/h3&gt;

&lt;p&gt;First, we take a look at the (log) mean squared error of our autoencoder over 30 iterations of IRL, given in the plot below.&lt;/p&gt;

&lt;figure&gt;

  &lt;picture&gt;
    
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/ae_10k_logloss-480.webp&quot; /&gt;
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/ae_10k_logloss-800.webp&quot; /&gt;
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/ae_10k_logloss-1400.webp&quot; /&gt;
    

    &lt;!-- Fallback to the original file --&gt;
    &lt;img src=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/ae_10k_logloss.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt;

  &lt;/picture&gt;

&lt;/figure&gt;

&lt;div class=&quot;caption&quot;&gt;
    Autoencoder log loss over IRL of 30 iterations.
&lt;/div&gt;

&lt;p&gt;We notice that both the train and validation loss steeply decrease until around iteration 10, upon which the validation loss begins to roughly stabilize and converge. This confirms our intuition that the loss following an iterated approach should eventually converge, which we can theoretically verify by observing that if we ran \(n\) iterations, then as \(n\to\infty\), because the loss is lower-bounded by zero and should generally from iteration to iteration (since we are removing information from our data), we must eventually converge. We further hypothesize that the fact that the loss has converged means that the embeddings upon convergence have learned the most succinct, critical portion of the data.&lt;/p&gt;

&lt;p&gt;We also notice that the number of iterations until convergence is very small; as mentioned, after about 10 iterations, it seems that the validation loss has roughly converged. We had hypothesized earlier that if the autoencoder converges after a small number of iterations, then that says something about the quality of the autoencoder architecture. Here, the fact that the loss converged after a small number iterations gives evidence for this hypothesis, since based on separate tests, this architecture indeed achieves relatively high classification accuracy for the MNIST dataset. We suggest that IRL can thus serve as a framework for evaluating the quality of an autoencoder on a particular dataset.&lt;/p&gt;

&lt;p&gt;Additionally, the validation loss converges at a relatively small number (around 0.25 by iteration 10), meaning that the distance between the original and reconstructed data in a given iteration are very similar. Interestingly enough, the validation loss is actually consistently lower than the train loss, which suggests that the learned representations through this iterated approach actually generalize very well to unseen data, which is certainly a desirable quality of any model.&lt;/p&gt;

&lt;p&gt;We also give the original and reconstructed data for iterations 1, 5, 10, 15, and 20, for both the train and test data, in the figures below.&lt;/p&gt;

&lt;figure&gt;

  &lt;picture&gt;
    
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/ae_10k_digits_train-480.webp&quot; /&gt;
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/ae_10k_digits_train-800.webp&quot; /&gt;
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/ae_10k_digits_train-1400.webp&quot; /&gt;
    

    &lt;!-- Fallback to the original file --&gt;
    &lt;img src=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/ae_10k_digits_train.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt;

  &lt;/picture&gt;

&lt;/figure&gt;

&lt;div class=&quot;caption&quot;&gt;
    Reconstructed train data.
&lt;/div&gt;

&lt;figure&gt;

  &lt;picture&gt;
    
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/ae_10k_digits_test-480.webp&quot; /&gt;
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/ae_10k_digits_test-800.webp&quot; /&gt;
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/ae_10k_digits_test-1400.webp&quot; /&gt;
    

    &lt;!-- Fallback to the original file --&gt;
    &lt;img src=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/ae_10k_digits_test.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt;

  &lt;/picture&gt;

&lt;/figure&gt;

&lt;div class=&quot;caption&quot;&gt;
    Reconstructed test data.
&lt;/div&gt;

&lt;p&gt;In the beginning, we see that the data starts losing resolution (e.g. the numbers become fuzzier and start losing their distinctness from the background), which makes sense because more iterations means more reconstructions that continue to accumulate reconstruction loss. The reconstructed images are also less clear than the originals due to the information that is lost from the encoding-decoding process.&lt;/p&gt;

&lt;p&gt;Our key observation is that the reconstruction loss stabilizes around the 10th iteration, where the original test images and reconstructed test images look very similar — we hypothesize that this is the point where the autoencoder has learned to represent the data as succinct as possible while preserving the most critical information.&lt;/p&gt;

&lt;h3 id=&quot;vae-irl-analysis&quot;&gt;VAE IRL Analysis&lt;/h3&gt;

&lt;p&gt;We similarly plot the log loss for our VAE, as well as the train, test, and sampled data over iterations in the figures below.&lt;/p&gt;

&lt;figure&gt;

  &lt;picture&gt;
    
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/vae_10k_logloss-480.webp&quot; /&gt;
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/vae_10k_logloss-800.webp&quot; /&gt;
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/vae_10k_logloss-1400.webp&quot; /&gt;
    

    &lt;!-- Fallback to the original file --&gt;
    &lt;img src=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/vae_10k_logloss.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt;

  &lt;/picture&gt;

&lt;/figure&gt;

&lt;div class=&quot;caption&quot;&gt;
    VAE log loss over IRL of 30 iterations.
&lt;/div&gt;

&lt;figure&gt;

  &lt;picture&gt;
    
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 480px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/vae_10k_digits-480.webp&quot; /&gt;
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 800px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/vae_10k_digits-800.webp&quot; /&gt;
    &lt;source class=&quot;responsive-img-srcset&quot; media=&quot;(max-width: 1400px)&quot; srcset=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/vae_10k_digits-1400.webp&quot; /&gt;
    

    &lt;!-- Fallback to the original file --&gt;
    &lt;img src=&quot;/staging/assets/img/2023-12-12-Iterated-Representation-Learning/vae_10k_digits.png&quot; class=&quot;img-fluid&quot; width=&quot;auto&quot; height=&quot;auto&quot; onerror=&quot;this.onerror=null; $(&apos;.responsive-img-srcset&apos;).remove();&quot; /&gt;

  &lt;/picture&gt;

&lt;/figure&gt;

&lt;div class=&quot;caption&quot;&gt;
    Train, test, and normal sampled data.
&lt;/div&gt;

&lt;p&gt;Unlike the autoencoder, the VAE’s train data becomes much more noisy across the 20 iterations. This is likely due to how the VAE injects noise in the reconstruction, which in this case resulted in the images to lose their distinctness. While the general shape is preserved (roundness, lines, etc), many of the numbers actually ended up merging together and losing their number shape altogether (e.g. some 6s, 3s, 9s all become 0s).&lt;/p&gt;

&lt;p&gt;When comparing IRL on the autoencoder versus the VAE, we observe that the VAE’s log loss converges to a larger log loss than the autoencoder, which makes sense because the VAE’s decoding step adds noise to the images that therefore adds loss to the reconstruction. We also note that the both of the models experience steep drop offs in log loss initially, which means the first few iterations eliminated most of the noise in the data and preserved the features that we characterize as “stable”.&lt;/p&gt;

&lt;h2 id=&quot;discussion&quot;&gt;Discussion&lt;/h2&gt;

&lt;p&gt;Our proposed IRL framework considers how some features may be more important or more stable than others, and it aims to capture those features while eliminating the noise in the data. While traditional dimensionality reduction techniques have their merits, IRL takes those methods one step further by iteratively trimming away noise until convergence or termination. Throughout this project, we cover representation learning fundamentals and IRL can capitalize on the way they learn embeddings, and we also apply this framework to real world data on MNIST. We argue that in our case study of MNIST, IRL does converge in terms of both loss (log mean squared error converges) and reconstructions, which is a promising first step in the analysis of stability and fundamental characteristics of the data. Moreover, we showcase how the number of iterations until convergence has significance, serving as a benchmark for how good an autoencoder/VAE is on a given dataset. Although VAE’s reconstructed images were more noisy, that’s by nature of the VAE, and we still observe that the fundamental features of the data (lines vs circles) are still preserved throughout iterations.&lt;/p&gt;

&lt;p&gt;There are a variety of directions we’d like to continue to explore with this project, given more time.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;We were only able to run a limited number of experiments due to computational power and the duration of time to train a full IRL from start to finish for, say, 30 iterations. Given more time, there are multiple other experiments we’d like to run, including training on other datasets and trying out the performance on different autoencoder architectures to better understand the properties of this iterated approach. Another thing we’d like to evaluate the empirical performance of, but also couldn’t due to computational constraints, is how a single autoencoder with 20 times as many neurons as some basic autoencoder compares to the basic autoencoder trained using IRL for 20 iterations.&lt;/li&gt;
  &lt;li&gt;We’re also curious to further explore the theoretical guarantees provided by IRL, including rigorous bounds on convergence. We’re also very interested in exploring whether any of our observations from IRL can generalize to other classes of deep learning models.&lt;/li&gt;
  &lt;li&gt;We’d lastly look into ways to make IRL more computationally tractable. As mentioned, our experimentation was heavily limited due to the computational cost of training a new autoencoder during every iteration. If possible, we’d like to look for optimizations of this framework that still preserve the desired methodology.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Overall, Iterated Representation Learning serves as a framework to evaluate stability-related properties of data, which we believe to be an important but overlooked standard for representation learning. Our case study of MNIST shows promise for empirical convergence guarantees on certain datasets, and we hope that our work lays the foundation for future representation discussions with respect to stability.&lt;/p&gt;
</content>
 </entry>
 

</feed>
